{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0PIPlV20XdzM",
    "outputId": "830d10db-7d03-4a6d-dfae-eca6d376c6d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (5.15.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly) (8.2.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly) (24.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ucy0mOYgqr1J",
    "outputId": "30907d37-57c2-45cc-98a0-c7ab03186d84"
   },
   "outputs": [],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pkGtxcy3qYUV",
    "outputId": "f16c2e55-fdbd-4649-b57e-46095ee2903e"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "VC2nPJZrPzwx"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "import json  # Добавьте этот импорт в начало вашего скрипта\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import statsmodels.api as sm  # Добавьте этот импорт в начало вашего файла\n",
    "from joblib import dump, load\n",
    "import zipfile\n",
    "import plotly.graph_objs as go\n",
    "import shutil\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "TOcrx6DVV-0d"
   },
   "outputs": [],
   "source": [
    "class MakeMyDataSet:\n",
    "    # Атрибут класса\n",
    "    _input_file = \"input.csv\"\n",
    "    _output_file = \"output.csv\"\n",
    "    applied_methods = {}\n",
    "    file_name_train = \"train_data.csv\"\n",
    "    file_name_test =  \"test_data.csv\"\n",
    "    file_name_json = \"methods_info.json\"\n",
    "    file_name_seasonal = \"seasonal.csv\"\n",
    "    file_name_trend = \"trend.csv\"\n",
    "    scaler_filename_x = \"scaler_x.joblib\"\n",
    "    scaler_filename_y = \"scaler_y.joblib\"\n",
    "\n",
    "    #устнавливаем входной файл, с которым будем работать (расширение txt)\n",
    "    def setNameInputFile(self, input_file):\n",
    "        self._input_file = input_file\n",
    "\n",
    "    def __add_prefix_to_filename(self, prefix):\n",
    "        # Разделяем путь на директорию и имя файла\n",
    "        directory, filename = os.path.split(self._input_file)\n",
    "\n",
    "        # Добавляем префикс к имени файла\n",
    "        new_filename = prefix + filename\n",
    "\n",
    "        # Соединяем директорию и новое имя файла обратно в полный путь\n",
    "        new_file_path = os.path.join(directory, new_filename)\n",
    "\n",
    "        return new_file_path\n",
    "\n",
    "    #устнавливаем выходные файлы (расширение csv)\n",
    "    def setNameOutputFile(self, prefix):\n",
    "        self._output_file = self.__add_prefix_to_filename(prefix)\n",
    "\n",
    "\n",
    "    #преобразование числовых значений в направления ветра\n",
    "    def __degrees_to_direction(self, degrees):\n",
    "        if degrees < 0 or degrees > 360:\n",
    "            return \"Недействительный угол\"\n",
    "        if 337.5 <= degrees or degrees < 22.5:\n",
    "            return \"Север\"\n",
    "        elif 22.5 <= degrees < 67.5:\n",
    "            return \"Северо-Восток\"\n",
    "        elif 67.5 <= degrees < 112.5:\n",
    "            return \"Восток\"\n",
    "        elif 112.5 <= degrees < 157.5:\n",
    "            return \"Юго-Восток\"\n",
    "        elif 157.5 <= degrees < 202.5:\n",
    "            return \"Юг\"\n",
    "        elif 202.5 <= degrees < 247.5:\n",
    "            return \"Юго-Запад\"\n",
    "        elif 247.5 <= degrees < 292.5:\n",
    "            return \"Запад\"\n",
    "        elif 292.5 <= degrees < 337.5:\n",
    "            return \"Северо-Запад\"\n",
    "\n",
    "    def __degrees_to_sin(self, degrees):\n",
    "        return np.sin(np.radians(degrees))\n",
    "\n",
    "    def __degrees_to_cos(self, degrees):\n",
    "        return np.cos(np.radians(degrees))\n",
    "\n",
    "    #добавляем столбцы sin, cos для флюгеров\n",
    "    def makeColumnsVane(self, file, columns_vane):\n",
    "       try:\n",
    "            data = pd.read_csv(file)\n",
    "            data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "            for column in data.columns:\n",
    "              if column in columns_vane:\n",
    "                data[column + '_sin'] = data[column].apply(self.__degrees_to_sin)\n",
    "                data[column + '_cos'] = data[column].apply(self.__degrees_to_cos)\n",
    "\n",
    "            data.to_csv(self._output_file, index=False)\n",
    "            self.applied_methods['makeColumnsVane'] = True\n",
    "       except FileNotFoundError:\n",
    "            print(f\"Ошибка: файл '{file}' не найден.\")\n",
    "            return\n",
    "       except Exception as e:\n",
    "            print(f\"Произошла ошибка: {e}\")\n",
    "            return\n",
    "\n",
    "    # Вычисление разницы времени между последовательными записями в минутах\n",
    "    def __find_time_diff_min(self, data):\n",
    "        # Создаем временный столбец для расчёта разницы времени\n",
    "        temp_time_diff = data['date'].diff().dt.total_seconds() / 60\n",
    "\n",
    "        # Находим наиболее частое значение разницы времени\n",
    "        mode_time_diff = temp_time_diff.mode()[0]\n",
    "\n",
    "        print(f\"Наиболее частая разница во времени между записями: {mode_time_diff} минут\")\n",
    "\n",
    "        # Возвращаем результат без изменения исходного DataFrame\n",
    "        return mode_time_diff\n",
    "\n",
    "    #сохранение сезонности с учетом даты начала периода\n",
    "    def __save_seasonality_with_dates(self, seasonal_component, period, filename):\n",
    "        try:\n",
    "            if not isinstance(seasonal_component.index, pd.DatetimeIndex):\n",
    "                raise ValueError(\"Индекс seasonal_component должен быть временным (DatetimeIndex).\")\n",
    "\n",
    "            # Извлечение первого периода сезонности вместе с индексом дат\n",
    "            first_period_seasonality = seasonal_component.head(int(period))\n",
    "\n",
    "            # Преобразование индекса в столбец для ясности\n",
    "            first_period_seasonality = first_period_seasonality.reset_index()\n",
    "\n",
    "            # Сохранение даты и сезонной компоненты в файл CSV\n",
    "            first_period_seasonality.to_csv(filename, index=False)\n",
    "\n",
    "            print(f\"Сезонная составляющая за первый период сохранена в '{filename}' с датами.\")\n",
    "\n",
    "        except ValueError as e:\n",
    "            print(f\"Ошибка в данных: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Произошла ошибка при сохранении сезонной составляющей: {e}\")\n",
    "\n",
    "\n",
    "    #убираем сезонность\n",
    "    def deleteSeason(self, file, column_goal = ''):\n",
    "        try:\n",
    "            data = pd.read_csv(file)\n",
    "            data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "            # Определение минимальной и максимальной даты\n",
    "            min_date = data['date'].min()\n",
    "            max_date = data['date'].max()\n",
    "\n",
    "            # Вычисление разницы в днях между максимальной и минимальной датами\n",
    "            date_diff = (max_date - min_date).days\n",
    "\n",
    "            # Вычисление разницы времени между последовательными записями в минутах\n",
    "            diff_time = self.__find_time_diff_min(data)\n",
    "            seasonal_period = 1\n",
    "\n",
    "            if diff_time!=0:\n",
    "              if date_diff>2*365: #надо как минимум 2 периода для выявления сезонности\n",
    "                #если год: 365 дней, 24 часа по 60 минут\n",
    "                seasonal_period = int(365*(24*60)/diff_time)\n",
    "                seasonal_period_text ='yearly'\n",
    "              else:\n",
    "                #если день: 24 часа по 60 минут\n",
    "                seasonal_period = int((24*60)/diff_time)\n",
    "                seasonal_period_text ='dayly'\n",
    "\n",
    "            model_rez = 'additive'\n",
    "\n",
    "            # Убедитесь, что 'date' является индексом DataFrame перед декомпозицией\n",
    "            data.set_index('date', inplace=True)\n",
    "\n",
    "            for column in data.columns:\n",
    "                decomposition = sm.tsa.seasonal_decompose(data[column],\n",
    "                                                          model=model_rez,\n",
    "                                                          period=seasonal_period,\n",
    "                                                          extrapolate_trend='freq')\n",
    "\n",
    "                seasonal_component = decomposition.seasonal\n",
    "\n",
    "                aligned_data, aligned_seasonal = data[column].align(decomposition.seasonal, join='left', fill_value=0)\n",
    "                data[column] = aligned_data - aligned_seasonal\n",
    "                data[column] = data[column] - decomposition.seasonal\n",
    "\n",
    "             # Возвращаем 'date' в столбцы DataFrame\n",
    "            data.reset_index(inplace=True)\n",
    "            data.to_csv(file, index=False)\n",
    "\n",
    "            self.__save_seasonality_with_dates(seasonal_component,\n",
    "                                             seasonal_period,\n",
    "                                             self.file_name_seasonal)\n",
    "\n",
    "            self.applied_methods['deleteSeason'] = { 'extrapolate_trend': 'freq',\n",
    "                                                    'model': model_rez,\n",
    "                                                    'period': seasonal_period_text,\n",
    "                                                    'seasonal_file': self.file_name_seasonal}\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Ошибка: файл '{file}' не найден.\")\n",
    "        except ValueError as e:\n",
    "            print(f\"Ошибка данных: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Произошла ошибка: {e}\")\n",
    "\n",
    "\n",
    "    def deleteSeasonForTest(self, file):\n",
    "        try:\n",
    "            if 'deleteSeason' not in self.applied_methods:\n",
    "                raise ValueError(\"Конфигурация сезонности не найдена.\")\n",
    "\n",
    "            data = pd.read_csv(file)\n",
    "            data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "            season_config = self.applied_methods['deleteSeason']\n",
    "            seasonal_file = season_config['seasonal_file']\n",
    "            period = season_config['period']\n",
    "            model = season_config['model']\n",
    "\n",
    "            # Загрузка сезонности\n",
    "            seasonality = pd.read_csv(seasonal_file, parse_dates=['date'])\n",
    "\n",
    "            if period == 'yearly':\n",
    "                seasonality['time_key'] = seasonality['date'].dt.dayofyear\n",
    "                if 'date' in data.columns:\n",
    "                    data['time_key'] = pd.to_datetime(data['date']).dt.dayofyear\n",
    "                else:\n",
    "                    raise ValueError(\"Столбец 'date' отсутствует в данных.\")\n",
    "            elif period == 'dayly':\n",
    "                 seasonality['time_key'] = seasonality['date'].dt.hour * 60 + seasonality['date'].dt.minute\n",
    "                 if 'date' in data.columns:\n",
    "                    # Конвертация времени в минуты с начала дня\n",
    "                    data['time_key'] = pd.to_datetime(data['date']).dt.hour * 60 + pd.to_datetime(data['date']).dt.minute\n",
    "                 else:\n",
    "                    raise ValueError(\"Столбец 'date' отсутствует в данных.\")\n",
    "\n",
    "            # Установка time_key как индекс в данных сезонности\n",
    "            seasonality.set_index('time_key', inplace=True)\n",
    "            data.set_index('time_key', inplace=True)\n",
    "\n",
    "            # Переиндексация основных данных для соответствия сезонности\n",
    "            aligned_seasonality = seasonality.reindex(data.index, method='nearest')\n",
    "            # Слияние с использованием merge_asof\n",
    "            #data = pd.merge_asof(data.sort_values('time_key'), seasonality.sort_values('time_key'), on='time_key', direction='nearest')\n",
    "\n",
    "            # Прибавление или умножение сезонной компоненты\n",
    "            #for column in data.select_dtypes(include=[np.number]):\n",
    "            #    if column in seasonality.columns:\n",
    "            #        if model == \"additive\":\n",
    "            #            data[column] += data[column + '_y'].fillna(0)\n",
    "            #        elif model == \"multiplicative\":\n",
    "            #            data[column] *= data[column + '_y'].fillna(1)\n",
    "\n",
    "\n",
    "            # Исключение дат из операций\n",
    "            numeric_columns = data.select_dtypes(include=[np.number])\n",
    "\n",
    "            # Прибавление или умножение сезонной компоненты к числовым данным\n",
    "            for column in numeric_columns:\n",
    "                if column in aligned_seasonality.columns:\n",
    "                    if model == \"additive\":\n",
    "                        data[column] += aligned_seasonality[column].fillna(0)\n",
    "                    elif model == \"multiplicative\":\n",
    "                        data[column] *= aligned_seasonality[column].fillna(1)\n",
    "\n",
    "            # Восстановление оригинального порядка столбцов и индекса\n",
    "            data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            data.to_csv(file, index=False)\n",
    "\n",
    "            print(\"Сезонная компонента успешно применена к данным.\")\n",
    "\n",
    "        except FileNotFoundError:\n",
    "             print(f\"Ошибка: файл '{self.file_name_seasonal}' не найден.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Произошла ошибка при применении сезонности: {e}\")\n",
    "\n",
    "    #убираем тренд\n",
    "    def deleteTrend(self, file):\n",
    "        try:\n",
    "            data = pd.read_csv(file)\n",
    "            data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d %H:%M:%S')\n",
    "            time = np.arange(len(data))\n",
    "\n",
    "            trend_info = {}\n",
    "\n",
    "            for column in data.columns:\n",
    "                if column == 'date':\n",
    "                    continue\n",
    "\n",
    "                # Создание модели линейной регрессии и вычисление тренда\n",
    "                y = data[column].values\n",
    "                model = LinearRegression().fit(time.reshape(-1, 1), y)\n",
    "\n",
    "                # Убираем тренд из данных\n",
    "                trend = model.predict(time.reshape(-1, 1))\n",
    "                data[column] = y - trend\n",
    "\n",
    "                # Сохранение информации о тренде, если это требуемый столбец\n",
    "                trend_info[column] = {'slope': model.coef_[0], 'intercept': model.intercept_}\n",
    "\n",
    "            # Сохранение обработанных данных\n",
    "            data.to_csv(file, index=False)\n",
    "\n",
    "            # Сохранение информации о тренде в файл, если есть что сохранять\n",
    "            if trend_info is not None:\n",
    "                pd.DataFrame(trend_info).to_csv(self.file_name_trend, index=True)\n",
    "                self.applied_methods['deleteTrend'] = {'trend_file': self.file_name_trend}\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Ошибка: файл '{file}' не найден.\")\n",
    "            return\n",
    "        except Exception as e:\n",
    "            print(f\"Произошла ошибка: {e}\")\n",
    "            return\n",
    "\n",
    "    #применяем дифференциование к данным\n",
    "    def applyDiffToData(self, file, is_write):\n",
    "        try:\n",
    "            data = pd.read_csv(file)\n",
    "\n",
    "            # Сохраняем первые значения для числовых столбцов\n",
    "            numeric_cols = data.select_dtypes(include=[np.number])\n",
    "            initial_values = numeric_cols.iloc[0]\n",
    "\n",
    "            # Применение дифференцирования только к числовым столбцам\n",
    "            diff_data = numeric_cols.diff(periods=1).bfill().dropna()\n",
    "\n",
    "            # Возвращаем нечисловые столбцы назад в DataFrame\n",
    "            non_numeric_cols = data.select_dtypes(exclude=[np.number])\n",
    "            diff_data = pd.concat([diff_data, non_numeric_cols], axis=1)\n",
    "\n",
    "            # Сохранение обработанных данных\n",
    "            diff_data.to_csv(file, index=False)\n",
    "            print(\"Данные дифференцированы\")\n",
    "\n",
    "            # Сохраняем первые значения в атрибуты класса или в файл, если это необходимо\n",
    "            if is_write:\n",
    "                self.applied_methods['applyDiffToData'] = {'initial_values': initial_values.to_dict()}\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Ошибка: файл '{file}' не найден.\")\n",
    "            return\n",
    "        except Exception as e:\n",
    "            print(f\"Произошла ошибка: {e}\")\n",
    "            return\n",
    "\n",
    "\n",
    "    #добавляем столбцы с датой в виде года/месяца\n",
    "    def transformColumnData(self, file, extract = 'year'):\n",
    "        try:\n",
    "            data = pd.read_csv(file)\n",
    "            data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "            min_year = data['date'].dt.year.min()\n",
    "            # Добавляем столбец с годом или месяцем\n",
    "            if extract == 'year':\n",
    "                data['year'] = data['date'].dt.year - min_year\n",
    "            elif extract == 'month':\n",
    "                data['month'] = data['date'].dt.month\n",
    "\n",
    "            # Сохранение обработанного DataFrame\n",
    "            data.to_csv(self._output_file, index=False)\n",
    "            self.applied_methods['transformColumnData'] = {'method': extract}\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Ошибка: файл '{file}' не найден.\")\n",
    "            return\n",
    "        except Exception as e:\n",
    "            print(f\"Произошла ошибка: {e}\")\n",
    "            return\n",
    "\n",
    "    #агрегация данных до значений\n",
    "    def agregateData(self, file, type_agregation = 'min'):\n",
    "       try:\n",
    "            # Загрузим данные из CSV файла.\n",
    "            data = pd.read_csv(file)\n",
    "            data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d %H:%M:%S')\n",
    "            data.set_index('date', inplace=True)\n",
    "\n",
    "            # Агрегация по 30 минутам:\n",
    "            if type_agregation == '30min':\n",
    "                resampled = data.resample('30min').mean()\n",
    "            elif type_agregation == '1hour':\n",
    "                # Агрегация по 1 часу:\n",
    "                resampled = data.resample('1h').mean()\n",
    "            elif type_agregation == '6hours':\n",
    "                # Агрегация по 6 часам:\n",
    "                resampled = data.resample('6h').mean()\n",
    "            elif type_agregation == '12hours':\n",
    "                # Агрегация по 12 часам:\n",
    "                resampled = data.resample('12h').mean()\n",
    "\n",
    "            self.applied_methods['agregateData'] = {'method': type_agregation}\n",
    "            # Теперь запишем результаты обратно в CSV.\n",
    "            resampled.to_csv(file)\n",
    "\n",
    "       except FileNotFoundError:\n",
    "           print(f\"Ошибка: файл '{file}' не найден.\")\n",
    "           return\n",
    "       except Exception as e:\n",
    "           print(f\"Произошла ошибка: {e}\")\n",
    "           return\n",
    "\n",
    "    #нормализуем данные\n",
    "    def normalizeData(self, file, column_goal='', method='minmax'):\n",
    "       try:\n",
    "           data = pd.read_csv(file)\n",
    "           dates = data['date']\n",
    "\n",
    "           # Выбор метода нормализации\n",
    "           scaler_x = MinMaxScaler() if method == 'minmax' else StandardScaler()\n",
    "           scaler_y = MinMaxScaler() if method == 'minmax' else StandardScaler()\n",
    "\n",
    "           # Нормализация целевого столбца, если он указан\n",
    "           if column_goal and column_goal in data.columns:\n",
    "                # Преобразование данных целевого столбца в 2D для fit_transform\n",
    "                column_data = data[[column_goal]]\n",
    "                data[column_goal] = scaler_y.fit_transform(column_data)\n",
    "                goal = data[column_goal]\n",
    "                dump(scaler_y, self.scaler_filename_y)\n",
    "                self.applied_methods['normalizeData_y'] = {'method': method,\n",
    "                                                           'scaler_file': self.scaler_filename_y}\n",
    "\n",
    "           # Нормализация всех столбцов, кроме 'date' и целевого\n",
    "           columns_to_scale = data.columns.drop(['date', column_goal]) if column_goal else data.columns.drop('date')\n",
    "           if columns_to_scale.empty:\n",
    "                print(\"Warning: No columns to scale!\")\n",
    "           else:\n",
    "                data[columns_to_scale] = scaler_x.fit_transform(data[columns_to_scale])\n",
    "                dump(scaler_x, self.scaler_filename_x)\n",
    "                self.applied_methods['normalizeData_x'] = {'method': method,\n",
    "                                                           'scaler_file': self.scaler_filename_x}\n",
    "\n",
    "\n",
    "\n",
    "           data['date'] = dates\n",
    "           data[column_goal] = goal\n",
    "\n",
    "           # Сохранение обработанного DataFrame\n",
    "           data.to_csv(file, index=False)\n",
    "\n",
    "       except FileNotFoundError:\n",
    "            print(f\"Ошибка: файл '{file}' не найден.\")\n",
    "            return\n",
    "       except Exception as e:\n",
    "            print(f\"Произошла ошибка: {e}\")\n",
    "            return\n",
    "\n",
    "     #применяем нормализацию для тестовых данных\n",
    "    def makeNormalizeForTest(self, file, column_goal):\n",
    "        try:\n",
    "           # Загрузка данных\n",
    "           data = pd.read_csv(file)\n",
    "           dates = data['date'].copy()\n",
    "\n",
    "\n",
    "           if os.path.exists(self.scaler_filename_y):\n",
    "                scaler_y = load(self.scaler_filename_y)\n",
    "                column_data = data[[column_goal]]\n",
    "                # Нормализация целевых столбцов\n",
    "                data[column_goal] = scaler_y.transform(column_data)\n",
    "                print(\"Данные успешно нормализованы с использованием сохранённых scaler_y.\")\n",
    "                goal = data[column_goal]\n",
    "\n",
    "           # Загрузка объектов scaler\n",
    "           if os.path.exists(self.scaler_filename_x):\n",
    "                scaler_x = load(self.scaler_filename_x)\n",
    "                # Нормализация всех столбцов, кроме 'date' и целевых столбцов\n",
    "                columns_to_scale = data.columns.drop(['date', column_goal]) if column_goal else data.columns.drop('date')\n",
    "                if columns_to_scale.empty:\n",
    "                    print(\"Warning: No columns to scale!\")\n",
    "                else:\n",
    "                    data[columns_to_scale] = scaler_x.transform(data[columns_to_scale])\n",
    "                    print(\"Данные успешно нормализованы с использованием сохранённых scaler_x.\")\n",
    "\n",
    "           data['date'] = dates\n",
    "           data[column_goal] = goal\n",
    "\n",
    "           data.to_csv(file, index=False)\n",
    "\n",
    "        except FileNotFoundError:\n",
    "           print(f\"Ошибка: файл '{file}' или scaler файлы не найдены.\")\n",
    "           return None\n",
    "        except Exception as e:\n",
    "           print(f\"Произошла ошибка: {e}\")\n",
    "           return None\n",
    "\n",
    "    # Функция для заполнения столбцов различными методами интерполяции\n",
    "    def __fillWithInterpol(self, series, type):\n",
    "            filled_series = series  # Инициализируем filled_series\n",
    "\n",
    "            if type == 'linear':\n",
    "                filled_series = series.interpolate(method='linear')\n",
    "            elif type == 'spline':\n",
    "                filled_series = series.interpolate(method='spline', order=3)\n",
    "            elif type == 'nearest':\n",
    "                filled_series = series.interpolate(method='nearest')\n",
    "            elif type == 'mean':\n",
    "                filled_series = series.fillna(series.mean())\n",
    "            elif type == 'median':\n",
    "                filled_series = series.fillna(series.median())\n",
    "            else:\n",
    "                filled_series = series.interpolate(method='linear')\n",
    "                print(f\"Тип интерполяции '{type}' не поддерживается. Поэтому был взят тип linear по умолчанию\")\n",
    "\n",
    "            return filled_series\n",
    "\n",
    "    # Заполняем пропущенные значения в датасете\n",
    "    def fillInterpol(self, file, type = 'linear'):\n",
    "        try:\n",
    "            data = pd.read_csv(file)\n",
    "            data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d %H:%M:%S')\n",
    "            # Установите столбец даты и времени как индекс DataFrame\n",
    "            data.set_index('date', inplace=True)\n",
    "\n",
    "            # Сортировка данных по индексу\n",
    "            data = data.sort_index()\n",
    "\n",
    "            # Генерация полной последовательности дат\n",
    "            full_range = pd.date_range(start=data.index.min(), end=data.index.max(), freq='10min')\n",
    "\n",
    "            # Расширение DataFrame для включения всех временных меток в диапазоне\n",
    "            data_full = data.reindex(full_range)\n",
    "\n",
    "            # Применение функции заполнения пропусков к объединенному DataFrame\n",
    "            for column in data.columns:\n",
    "              data_full[column] = self.__fillWithInterpol(data_full[column], type)\n",
    "\n",
    "            # Сброс индекса перед сохранением в CSV\n",
    "            data_full.reset_index(inplace=True)\n",
    "\n",
    "            # Переименование столбца индекса обратно в 'date'\n",
    "            data_full.rename(columns={'index': 'date'}, inplace=True)\n",
    "\n",
    "            # Сохранение обработанного DataFrame в обратно в файл CSV\n",
    "            data_full.to_csv(self._output_file, index=False)\n",
    "            self.applied_methods['fillInterpol'] = {'method': type}\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Ошибка: файл '{self._output_file}' не найден.\")\n",
    "            return\n",
    "        except Exception as e:\n",
    "            print(f\"Произошла ошибка: {e}\")\n",
    "            return\n",
    "\n",
    "\n",
    "    # Делаем датасет со списоком столбцов, которые хотим выбрать\n",
    "    def makeNewFileDataset(self, columns_to_select):\n",
    "        try:\n",
    "            # Убеждаемся, что 'date' всегда включен в список столбцов для выборки\n",
    "            columns_to_select = list(set(columns_to_select + ['date']))\n",
    "\n",
    "            # Чтение CSV файла\n",
    "            data = pd.read_csv(self._input_file, usecols=columns_to_select)\n",
    "\n",
    "            # Сохранение обработанного DataFrame в CSV\n",
    "            data.to_csv(self._output_file, index=False)\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Ошибка: файл '{self._output_file}' не найден.\")\n",
    "            return\n",
    "        except Exception as e:\n",
    "            print(f\"Произошла ошибка: {e}\")\n",
    "            return\n",
    "\n",
    "    # Удаляем дату из данных\n",
    "    def deleteDateFromDataset(self, file):\n",
    "        try:\n",
    "           data = pd.read_csv(file)\n",
    "           if 'date' in data.columns:\n",
    "               dates = data['date'].copy()\n",
    "               data.drop('date', axis=1, inplace=True)  # Удаление столбца 'date' из DataFrame\n",
    "               # Сохранение обработанного DataFrame в CSV\n",
    "               data.to_csv(file, index=False)\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Ошибка: файл '{self._output_file}' не найден.\")\n",
    "            return\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Произошла ошибка: {e}\")\n",
    "            return\n",
    "\n",
    "    #сохраняем в методы название целевого столбца\n",
    "    def add_goal_column_name_to_method(self, goal_radio_group):\n",
    "        self.applied_methods['column_name_goal'] = goal_radio_group\n",
    "    \n",
    "    # Метод для сохранения словаря в JSON\n",
    "    def save_methods_to_json(self, json_filename):\n",
    "        with open(json_filename, 'w') as file:\n",
    "            json.dump(self.applied_methods, file, indent=4)\n",
    "\n",
    "    # Разделение датасета на тренировочную и тестовую выборки\n",
    "    def splitData(self, test_size=0.2):\n",
    "       try:\n",
    "            data = pd.read_csv(self._output_file)\n",
    "\n",
    "            # Определяем точку разделения\n",
    "            split_point = int(len(data) * (1 - test_size))\n",
    "\n",
    "            # Разделение данных на тренировочную и тестовую выборки\n",
    "            train_data = data[:split_point]\n",
    "            test_data = data[split_point:]\n",
    "\n",
    "            # Сохранение выборок в разные файлы\n",
    "            train_data.to_csv(self.file_name_train, index=False)\n",
    "            test_data.to_csv(self.file_name_test, index=False)\n",
    "            #test_data.to_csv(\"test_for_test.csv\", index=False)\n",
    "\n",
    "       except FileNotFoundError:\n",
    "            print(f\"Ошибка: файл '{self._input_file}' не найден.\")\n",
    "            return\n",
    "       except Exception as e:\n",
    "            print(f\"Произошла ошибка: {e}\")\n",
    "            return\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "FUY-K12CQiqM"
   },
   "outputs": [],
   "source": [
    "#класс для применения методов к данным\n",
    "class ApplyMethodsToData:\n",
    "    def __init__(self, initial_data, file_path):\n",
    "        self.data = initial_data.copy()\n",
    "        self.file_path = file_path\n",
    "        if 'date' in self.data.columns:\n",
    "            self.data['date'] = pd.to_datetime(self.data['date'], format='%Y-%m-%d %H:%M:%S')\n",
    "            #self.data.set_index('date', inplace=True)\n",
    "        self.methods = {}\n",
    "        self.sampling_window_size = None\n",
    "        self.test_data_offset = None\n",
    "        self.column_name_goal = None  \n",
    "\n",
    "    #Загрузка методов обработки из JSON файла.\n",
    "    def load_methods_from_json(self, json_filename):\n",
    "        try:\n",
    "            with open(json_filename, 'r') as file:\n",
    "                self.methods = json.load(file)\n",
    "                # Чтение дополнительных параметров\n",
    "            self.sampling_window_size = self.methods.get(\"sampling_window_size\", None)\n",
    "            self.test_data_offset = self.methods.get(\"test_data_offset\", None)\n",
    "            self.column_name_goal = self.methods.get(\"column_name_goal\", None)\n",
    "            print(f\"Loaded sampling_window_size: {self.sampling_window_size}\")\n",
    "            print(f\"Loaded test_data_offset: {self.test_data_offset}\")\n",
    "            print(f\"Loaded column_name_goalt: {self.column_name_goal}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Ошибка: файл '{json_filename}' не найден.\")\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Ошибка: файл '{json_filename}' содержит некорректный JSON.\")\n",
    "\n",
    "    #Обработка методов в обратном порядке в соответствии с загруженным JSON.\"\"\"\n",
    "    def process_methods(self, test = 0):\n",
    "        #for predict\n",
    "        methods_dict = {\n",
    "                            \"normalizeData_y\": self.applyNormalizeToData,\n",
    "                            \"deleteTrend\": self.applyTrendToData,\n",
    "                            \"deleteSeason\": self.applySeasonToData,\n",
    "                            \"applyDiffToData\": self.applyUnDiffToData\n",
    "                           }\n",
    "\n",
    "        # Обработка методов в обратном порядке\n",
    "        for method, details in reversed(list(self.methods.items())):\n",
    "            if method in methods_dict:\n",
    "                if isinstance(details, dict):  # Проверяем, являются ли детали словарём\n",
    "                    print(f\"Применение {method} с параметрами {details}...\")\n",
    "                    methods_dict[method](**details)  # Распаковка словаря в аргументы функции\n",
    "                else:\n",
    "                    print(f\"Применение {method}...\")\n",
    "                    methods_dict[method]()  # Вызов метода без параметров\n",
    "\n",
    "\n",
    "    #применить нормализацию к данным\n",
    "    def applyNormalizeToData(self, scaler_file, method):\n",
    "       try:\n",
    "\n",
    "            # Загрузка объекта scaler\n",
    "            scaler = load(os.path.join(self.file_path, scaler_file))\n",
    "\n",
    "            # Сохранение столбца с датами, если он есть\n",
    "            is_data = 0\n",
    "            if 'date' in self.data.columns:\n",
    "                is_data = 1\n",
    "                dates = self.data['date'].copy()\n",
    "\n",
    "            # Определение столбцов для нормализации\n",
    "            columns_to_scale = self.data.columns.drop('date', errors='ignore')  # Игнорируем ошибки, если 'date' нет\n",
    "            print(columns_to_scale)\n",
    "            # Применение нормализации\n",
    "            temp = self.data[columns_to_scale]\n",
    "            temp = scaler.inverse_transform(temp)\n",
    "\n",
    "            self.data[columns_to_scale] = temp\n",
    "            # Возвращение столбца с датами обратно в DataFrame, если он был сохранён\n",
    "            if is_data:\n",
    "                self.data['date'] = dates\n",
    "            print(\"Данные успешно восстановлены из нормализованной формы.\")\n",
    "\n",
    "       except FileNotFoundError:\n",
    "            print(f\"Ошибка: файл '{scaler_file}' не найден.\")\n",
    "       except Exception as e:\n",
    "            print(f\"Произошла ошибка: {e}\")\n",
    "\n",
    "\n",
    "    #применить обратную дифферециацию к данным\n",
    "    def applyUnDiffToData(self, initial_values):\n",
    "        try:\n",
    "            # Создаем копию DataFrame, чтобы избежать изменения исходных данных\n",
    "            numeric_data = self.data.copy()\n",
    "\n",
    "            # Удаляем столбец с датами из данных, предназначенных для восстановления\n",
    "            pr_restore = 0\n",
    "            if 'date' in numeric_data.columns:\n",
    "                pr_restore = 1\n",
    "                dates = numeric_data['date']\n",
    "                numeric_data.drop('date', axis=1, inplace=True)\n",
    "\n",
    "            # Восстановление числовых данных из дифференцированных значений\n",
    "            restored_data = numeric_data.cumsum() + initial_values\n",
    "\n",
    "            # Возвращаем столбец с датами обратно в DataFrame\n",
    "            if pr_restore:\n",
    "                restored_data['date'] = dates\n",
    "\n",
    "            # Поскольку cumsum() не включает первую строку начальных значений, заменяем первую строку на начальные значения\n",
    "            for col in restored_data.columns:\n",
    "                if col != 'date':\n",
    "                    restored_data[col].iloc[0] = initial_values[col]\n",
    "\n",
    "            # Сохранение восстановленных данных\n",
    "            self.data = restored_data\n",
    "            print(\"Данные успешно восстановлены из дифференцированной формы.\")\n",
    "\n",
    "        except Exception as e:\n",
    "                print(f\"Произошла ошибка: {e}\")\n",
    "\n",
    "    #применить тренд к данным\n",
    "    def applyTrendToData(self, trend_file):\n",
    "        try:\n",
    "            # Загрузка информации о тренде\n",
    "            trend_info = pd.read_csv(os.path.join(self.file_path,trend_file), index_col=0)\n",
    "            time = np.arange(len(self.data))\n",
    "\n",
    "            # Применение тренда ко всем столбцам, для которых он был сохранен\n",
    "            for column in trend_info.columns:\n",
    "                if column in self.data.columns:\n",
    "                    # Извлечение параметров тренда\n",
    "                    slope = trend_info.at['slope', column]\n",
    "                    intercept = trend_info.at['intercept', column]\n",
    "\n",
    "                    # Вычисление тренда\n",
    "                    trend = slope * time + intercept\n",
    "\n",
    "                    # Добавление тренда к данным\n",
    "                    self.data[column] = self.data[column] + trend\n",
    "                    print(f\"Тренд успешно применён к столбцу {column}.\")\n",
    "\n",
    "        except FileNotFoundError:\n",
    "                print(f\"Ошибка: файл '{trend_file}' не найден.\")\n",
    "        except Exception as e:\n",
    "                print(f\"Произошла ошибка: {e}\")\n",
    "\n",
    "    #применить сезонность к данным\n",
    "    def applySeasonToData(self, period, seasonal_file, model, extrapolate_trend):\n",
    "        try:\n",
    "            # Загрузка сезонности\n",
    "            seasonality = pd.read_csv(os.path.join(self.file_path,seasonal_file), parse_dates=['date'])\n",
    "            if period == 'yearly':\n",
    "                seasonality['time_key'] = seasonality['date'].dt.dayofyear\n",
    "                if 'date' in self.data.columns:\n",
    "                    self.data['time_key'] = pd.to_datetime(self.data['date']).dt.dayofyear\n",
    "                else:\n",
    "                    raise ValueError(\"Столбец 'date' отсутствует в данных.\")\n",
    "            elif period == 'dayly':\n",
    "                 seasonality['time_key'] = seasonality['date'].dt.hour * 60 + seasonality['date'].dt.minute\n",
    "                 if 'date' in self.data.columns:\n",
    "                    # Конвертация времени в минуты с начала дня\n",
    "                    self.data['time_key'] = pd.to_datetime(self.data['date']).dt.hour * 60 + pd.to_datetime(self.data['date']).dt.minute\n",
    "                 else:\n",
    "                    raise ValueError(\"Столбец 'date' отсутствует в данных.\")\n",
    "\n",
    "            # Установка time_key как индекс в данных сезонности\n",
    "            seasonality.set_index('time_key', inplace=True)\n",
    "            self.data.set_index('time_key', inplace=True)\n",
    "\n",
    "            # Переиндексация основных данных для соответствия сезонности\n",
    "            aligned_seasonality = seasonality.reindex(self.data.index, method='nearest')\n",
    "\n",
    "            # Исключение дат из операций\n",
    "            numeric_columns = self.data.select_dtypes(include=[np.number])\n",
    "\n",
    "            # Прибавление или умножение сезонной компоненты к числовым данным\n",
    "            for column in numeric_columns:\n",
    "                if column in aligned_seasonality.columns:\n",
    "                    if model == \"additive\":\n",
    "                        self.data[column] += aligned_seasonality[column].fillna(0)\n",
    "                    elif model == \"multiplicative\":\n",
    "                        self.data[column] *= aligned_seasonality[column].fillna(1)\n",
    "\n",
    "            # Восстановление оригинального порядка столбцов и индекса\n",
    "            self.data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            print(\"Сезонная компонента успешно применена к данным.\")\n",
    "\n",
    "        except FileNotFoundError:\n",
    "             print(f\"Ошибка: файл '{seasonal_file}' не найден.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Произошла ошибка при применении сезонности: {e}\")\n",
    "\n",
    "    def saveToCSV(self, file_name_test):\n",
    "        if 'date' in self.data.columns:\n",
    "               dates = self.data['date'].copy()\n",
    "               self.data.drop('date', axis=1, inplace=True)  # Удаление столбца 'date' из DataFrame\n",
    "\n",
    "        self.data.to_csv(file_name_test, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "5ZhAlIP3P7EF"
   },
   "outputs": [],
   "source": [
    "# Стили CSS для настройки фона и текста\n",
    "css = \"\"\"\n",
    ".gradio-container {\n",
    "    background: none !important;\n",
    "    background-image: url('/content/vetroyenergetika.jpg') !important;\n",
    "    background-position: center;\n",
    "    background-repeat: no-repeat;\n",
    "    background-size: cover;\n",
    "    color: #4CAF50;\n",
    "    background-color: black !important;\n",
    "}\n",
    "\n",
    "h3, h2{\n",
    "   color: #4CAF50; /* Зеленый цвет текста */\n",
    "   font-family: Arial, sans-serif;\n",
    "}\n",
    "\n",
    ".button_my {\n",
    "    background: none !important;\n",
    "    background-color: #4CAF50 !important;\n",
    "    color: black;\n",
    "    padding: 10px 20px;\n",
    "    border: none;\n",
    "    border-radius: 5px;\n",
    "    cursor: pointer;\n",
    "    font-size: 16px;\n",
    "}\n",
    "\n",
    ".button_my:hover {\n",
    "    background-color: #70bf73 !important;\n",
    "}\n",
    "\n",
    ".tab_my {\n",
    "    background: none !important;\n",
    "    background-image: url('/content/images.jpeg') !important;\n",
    "    border: 1px solid #ddd !important; /* Граница вкладок */\n",
    "    padding: 10px !important; /* Отступы внутри вкладок */\n",
    "}\n",
    "\n",
    ".small-file-input {\n",
    "    width: 150px; /* Устанавливаем ширину */\n",
    "    height: 80px !important; /* Устанавливаем высоту */\n",
    "}\n",
    "\n",
    "upload_instructions {\n",
    "    font-size: 16px; /* Изменяем размер шрифта */\n",
    "}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "WEHpCWgha7Ty"
   },
   "outputs": [],
   "source": [
    "# Глобальные переменные для хранения данных и модели\n",
    "global_data = None\n",
    "\n",
    "# Словари модлей для разных вышек\n",
    "\n",
    "model_choose_telichki = {\n",
    "    'CNN': 'array_model/model_cnn_telichki.zip',\n",
    "    'RNN': 'array_model/model_rnn_telichki.zip',\n",
    "    'LSTM': 'array_model/model_lstm_telichki.zip',\n",
    "    'GRU': 'array_model/model_gru_telichki.zip'\n",
    "}\n",
    "\n",
    "model_choose_ust_har = {\n",
    "    'CNN': 'array_model/model_cnn_ust_harizovo.zip',\n",
    "    'RNN': 'array_model/model_rnn_ust_harizovo.zip',\n",
    "    'LSTM': 'array_model/model_lstm_ust_harizovo.zip',\n",
    "    'GRU': 'array_model/model_gru_ust_harizovo.zip'\n",
    "}\n",
    "\n",
    "model_choose_ossora = {\n",
    "    'CNN': 'array_model/model_cnn_ossora.zip',\n",
    "    'RNN': 'array_model/model_rnn_ossora.zip',\n",
    "    'LSTM': 'array_model/model_lstm_ossora.zip',\n",
    "    'GRU': 'array_model/model_gru_ossora.zip'\n",
    "}\n",
    "\n",
    "model_choose_manila = {\n",
    "    'CNN': 'array_model/model_cnn_manila.zip',\n",
    "    'RNN': 'array_model/model_rnn_manila.zip',\n",
    "    'LSTM': 'array_model/model_lstm_manila.zip',\n",
    "    'GRU': 'array_model/model_gru_manila.zip'\n",
    "}\n",
    "\n",
    "model_choose_palana = {\n",
    "    'CNN': 'array_model/model_cnn_palana.zip',\n",
    "    'RNN': 'array_model/model_rnn_palana.zip',\n",
    "    'LSTM': 'array_model/model_lstm_palana.zip',\n",
    "    'GRU': 'array_model/model_gru_palana.zip'\n",
    "}\n",
    "\n",
    "model_choose_ust_kam = {\n",
    "    'CNN': 'array_model/model_cnn_ust_kam.zip',\n",
    "    'RNN': 'array_model/model_rnn_ust_kam.zip',\n",
    "    'LSTM': 'array_model/model_lstm_ust_kam.zip',\n",
    "    'GRU': 'array_model/model_gru_ust_kam.zip'\n",
    "}\n",
    "\n",
    "\n",
    "# Загрузка моделей и создание словаря\n",
    "models = {\n",
    "    'Телички': model_choose_telichki,\n",
    "    'Усть-Харизово': model_choose_ust_har,\n",
    "    'Оссора': model_choose_ossora,\n",
    "    'Манила': model_choose_manila,\n",
    "    'Палана': model_choose_palana,\n",
    "    'Усть-Камчатск': model_choose_ust_kam\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "mWtuwh8YPyei"
   },
   "outputs": [],
   "source": [
    "# Словарь методов заполнения пропусков\n",
    "methods = {\n",
    "    'Линейная интерполяция': 'linear',\n",
    "    'Сплайн-интерполяция':'spline',\n",
    "    'Интерполяция ближайшими значениями':'nearest',\n",
    "    'Заполнение средним значением ':'mean',\n",
    "    'Заполнение медианным значением ':'median'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "4s-UbsPLAxV8"
   },
   "outputs": [],
   "source": [
    "# Словарь методов scaler\n",
    "normalize = {\n",
    "    'MinMaxScaler': 'minmax',\n",
    "    'StandartScaler':'standart'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "NPNTxLdHwWi9"
   },
   "outputs": [],
   "source": [
    "# Словарь методов трансформации даты\n",
    "transform_data_methods = {\n",
    "    'Номер месяца': 'month',\n",
    "    'Номер года':'year'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "qHcBS4ivcZfL"
   },
   "outputs": [],
   "source": [
    "# Словарь методов трансформации даты\n",
    "agregation_list = {\n",
    "    '30 минут': '30min',\n",
    "    '1 час':'1hour',\n",
    "    '6 часов':'6hours',\n",
    "    '12 часов':'12hours'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "kJrTi9iBzcXR"
   },
   "outputs": [],
   "source": [
    "#Папка, куда будут разархивироваться данные\n",
    "destination_folder = \"temp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "JxwFLAbIkTGy"
   },
   "outputs": [],
   "source": [
    "#находим json файл в папке\n",
    "def find_first_json_file(directory):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.json'):\n",
    "                return os.path.join(root, file)  # Возвращает путь к первому найденному файлу JSON и останавливает поиск\n",
    "    return None  # Если JSON файл не найден, вернуть None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "dK7HoqdClw3Y"
   },
   "outputs": [],
   "source": [
    "#находим модель h5, keras файл в папке\n",
    "def find_first_model_file(directory):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.h5') or file.endswith('.keras'):\n",
    "                return os.path.join(root, file)  # Возвращает путь к первой найденной модели и останавливает поиск\n",
    "    return None  # Если файл модели не найден, вернуть None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "eyosYzH4P-43"
   },
   "outputs": [],
   "source": [
    "#загружаем необходимые данные для прогнозирования\n",
    "def load_dataset(tower, model_name, file):\n",
    "\n",
    "    global global_data\n",
    "    global json_file_path\n",
    "    global sampling_window_size\n",
    "    global test_data_offset\n",
    "    global column_name_goal\n",
    "\n",
    "    #проверяем выбрана ли башня\n",
    "    if tower not in models:\n",
    "        return \"Башня не выбрана\"\n",
    "\n",
    "    # проверяем модель для башни, берем ссылку на архив\n",
    "    if model_name in models[tower]:\n",
    "        zip_path = models[tower][model_name]\n",
    "    else:\n",
    "        return \"Данные для модели не загружены\"\n",
    "\n",
    "    # Удаление папки, если она существует\n",
    "    if os.path.exists(destination_folder):\n",
    "        shutil.rmtree(destination_folder)\n",
    "    \n",
    "    # Создание папки\n",
    "    os.makedirs(destination_folder)\n",
    " \n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "      zip_ref.extractall(destination_folder)\n",
    "\n",
    "    # Найти первый JSON файл в этой папке\n",
    "    json_file_path = find_first_json_file(destination_folder)\n",
    "    if json_file_path:\n",
    "        print(f\"Найден JSON файл: {json_file_path}\")\n",
    "    else:\n",
    "        return \"JSON файл не найден.\"\n",
    "\n",
    "    # Загрузка тестового файла\n",
    "    if file.name.endswith('.csv'):\n",
    "        # Обработка CSV файла\n",
    "        data = pd.read_csv(file)\n",
    "    else:\n",
    "        return \"Неподдерживаемый формат файла\"\n",
    "\n",
    "    global_data = data.copy()  # Сохраните загруженные данные в глобальной переменной\n",
    "    global_data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "\n",
    "    #применяем методы в обратном порядке\n",
    "    processor = ApplyMethodsToData(data, destination_folder)\n",
    "    processor.load_methods_from_json(json_file_path)\n",
    "    processor.process_methods()\n",
    "    sampling_window_size = processor.sampling_window_size\n",
    "    test_data_offset = processor.test_data_offset\n",
    "    column_name_goal = processor.column_name_goal\n",
    "    \n",
    "    data_processor = processor.data\n",
    "\n",
    "    # Создание графика\n",
    "    if 'date' in data.columns:\n",
    "         data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "         data.set_index('date', inplace=True)\n",
    "\n",
    "    # Установка столбца с датой как индекса DataFrame\n",
    "    if 'date' in data_processor.columns:\n",
    "        data_processor['date'] = pd.to_datetime(data_processor['date'], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "        data_processor.set_index('date', inplace=True)\n",
    "    else:\n",
    "        return \"Столбец с датой не найден\"\n",
    "\n",
    "    #data_test = pd.read_csv(\"test_for_test.csv\")\n",
    "    #if 'date' in data_test.columns:\n",
    "    #    data_test['date'] = pd.to_datetime(data_test['date'], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "    #    data_test.set_index('date', inplace=True)\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Добавление исходных данных на график\n",
    "    for column in data.columns:\n",
    "        fig.add_trace(go.Scatter(x=data.index, y=data[column],\n",
    "                                 mode='lines', name=f'Original - {column}'))\n",
    "\n",
    "\n",
    "    # Добавление обработанных данных на график\n",
    "    for column in data_processor.columns:\n",
    "        fig.add_trace(go.Scatter(x=data_processor.index, y=data_processor[column],\n",
    "                                 mode='lines', name=f'Processed - {column}'))\n",
    "\n",
    "\n",
    "    # Добавление обработанных данных на график\n",
    "    #for column in data_test.columns:\n",
    "    #    fig.add_trace(go.Scatter(x=data_test.index, y=data_test[column],\n",
    "    #                             mode='lines', name=f'Test - {column}'))\n",
    "\n",
    "    fig.update_layout(title='Сравнение исходных и обработанных данных',\n",
    "                      xaxis_title='Дата',\n",
    "                      yaxis_title='Значения',\n",
    "                      legend_title='Легенда')\n",
    "\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для создания окон данных\n",
    "def create_dataset(data, n_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - n_steps):\n",
    "        X.append(data[i:i + n_steps])\n",
    "        y.append(data[i + n_steps])\n",
    "    return np.array(X), np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "p_7ZuWgRQCq1"
   },
   "outputs": [],
   "source": [
    "#делаем предсказание модели\n",
    "def make_prediction(duration):\n",
    "    global global_data\n",
    "    global json_file_path\n",
    "    global sampling_window_size\n",
    "    global test_data_offset\n",
    "    global column_name_goal\n",
    "\n",
    "    message = ''\n",
    "\n",
    "    # Найти первый файл модели в этой папке\n",
    "    model_file_path = find_first_model_file(destination_folder)\n",
    "    if model_file_path:\n",
    "        print(f\"Найден файл модели: {model_file_path}\")\n",
    "        model = load_model(model_file_path)        \n",
    "    else:\n",
    "        print(\"Файл модели не найден.\")\n",
    "        model = None\n",
    "\n",
    "    if model is None or global_data is None:\n",
    "        return \"Модель или данные не загружены\"\n",
    "\n",
    "    # Создаем временный столбец для расчёта разницы времени\n",
    "    temp_time_diff = global_data['date'].diff().dt.total_seconds() / 60\n",
    "\n",
    "    # Находим наиболее частое значение разницы времени в минутах\n",
    "    mode_time_diff = temp_time_diff.mode()[0]\n",
    "\n",
    "    #узнаем, сколько в одном дню должно быть подсчетов\n",
    "    N = int(24*60/mode_time_diff);\n",
    "    # предсказание на N шагов\n",
    "    input_data = global_data.iloc[test_data_offset:duration*N]\n",
    "\n",
    "    if 'date' in global_data.columns:\n",
    "        dates = input_data['date'].copy()  # Создаем копию столбца с датами\n",
    "        input_data = input_data.drop('date', axis=1)  # Удаляем столбец с датами из основного DataFrame\n",
    "\n",
    "    # Подготовка данных\n",
    "    X, _ = create_dataset(input_data[column_name_goal].values, sampling_window_size)\n",
    "\n",
    "    # Решейпинг X \n",
    "    X = X.reshape((X.shape[0], sampling_window_size, 1))\n",
    "\n",
    "    # Прогнозирование\n",
    "    prediction_df = model.predict(X)\n",
    "    prediction_df = pd.DataFrame(prediction_df, columns=[column_name_goal])\n",
    "        \n",
    "    # Получение предсказаний из DataFrame\n",
    "    predictions = prediction_df[column_name_goal].values \n",
    "    input_data = input_data.iloc[sampling_window_size:]\n",
    "    y_true = input_data[column_name_goal].values\n",
    " \n",
    "    print(len(y_true), len(predictions))\n",
    "    \n",
    "    # Расчет MAE\n",
    "    mae = mean_absolute_error(y_true, predictions)\n",
    "    print('Средняя абсолютная ошибка (MAE):', mae)\n",
    "    \n",
    "    # Расчет MSE\n",
    "    mse = mean_squared_error(y_true, predictions)\n",
    "    print('Среднеквадратичная ошибка (MSE):', mse)\n",
    "    \n",
    "    #применяем методы в обратном порядке\n",
    "    processor = ApplyMethodsToData(prediction_df, destination_folder)\n",
    "    processor.load_methods_from_json(json_file_path)\n",
    "    processor.process_methods()\n",
    "    data_processor = processor.data\n",
    "\n",
    "    processor_test = ApplyMethodsToData(input_data, destination_folder)\n",
    "    processor_test.load_methods_from_json(json_file_path)\n",
    "    processor_test.process_methods()\n",
    "    data_processor_test = processor_test.data\n",
    "\n",
    "    dates = dates[sampling_window_size:].reset_index(drop=True) \n",
    "    # Предполагаем, что `dates` имеет нужную длину. Если `prediction_df` был изменен, убедитесь, что даты соответствуют.\n",
    "    prediction_df['date'] = dates # Сбрасываем индекс на случай, если это нужно\n",
    "    prediction_df.set_index('date', inplace=True)\n",
    "    \n",
    "    # Аналогично для data_processor, если у вас есть обработанные данные\n",
    "    data_processor['date'] = dates\n",
    "    data_processor.set_index('date', inplace=True)\n",
    "\n",
    "    data_processor_test = data_processor_test.reset_index(drop=True)\n",
    "    data_processor_test['date'] = dates  # Сбрасываем индекс на случай, если это нужно\n",
    "    data_processor_test.set_index('date', inplace=True)\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Добавление не обработанных предсказанных данных на график\n",
    "    for column in prediction_df.columns:\n",
    "        fig.add_trace(go.Scatter(x=prediction_df.index, y=prediction_df[column],\n",
    "                                 mode='lines', name=f'Predict - {column}'))\n",
    "\n",
    "\n",
    "    # Добавление обработанных предсказанных данных на график\n",
    "    for column in data_processor.columns:\n",
    "        fig.add_trace(go.Scatter(x=data_processor.index, y=data_processor[column],\n",
    "                                 mode='lines', name=f'Predict recovery - {column}'))\n",
    "\n",
    "    # Добавление истинных данных на график\n",
    "    for column in data_processor_test.columns:\n",
    "        fig.add_trace(go.Scatter(x=data_processor_test.index, y=data_processor_test[column],\n",
    "                                 mode='lines', name=f'Test - {column}'))\n",
    "\n",
    "\n",
    "    fig.update_layout(title='Сравнение исходных и обработанных данных',\n",
    "                      xaxis_title='Дата',\n",
    "                      yaxis_title='Значения',\n",
    "                      legend_title='Легенда')\n",
    "\n",
    "\n",
    "    return fig, mae, mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "oO8nmRw15kZ2"
   },
   "outputs": [],
   "source": [
    "def update_models_dropdown(tower):\n",
    "    if tower in models:\n",
    "        # Получаем список моделей для выбранной вышки\n",
    "        model_names = models.get(tower, [])\n",
    "        return gr.Dropdown(label=\"\", choices=list(model_names.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "RHJkq4fckmrb"
   },
   "outputs": [],
   "source": [
    "def show_methods(checkbox_value):\n",
    "    if checkbox_value:\n",
    "        return gr.Dropdown(label=\"Выберите метод\", choices=list(methods.keys()), visible=True)\n",
    "    else:\n",
    "        # Скрыть выпадающий список, если чекбокс не отмечен\n",
    "        return gr.Dropdown(label=\"Выберите метод\", choices=list(methods.keys()), visible=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "IikQNdNQCDIt"
   },
   "outputs": [],
   "source": [
    "def show_normalize(checkbox_value):\n",
    "    if checkbox_value:\n",
    "        return gr.Dropdown(label=\"Выберите метод\", choices=list(normalize.keys()), visible=True)\n",
    "    else:\n",
    "        # Скрыть выпадающий список, если чекбокс не отмечен\n",
    "        return gr.Dropdown(label=\"Выберите метод\", choices=list(normalize.keys()), visible=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "m2bb90B9cZfM"
   },
   "outputs": [],
   "source": [
    "def show_agregation(checkbox_value):\n",
    "    if checkbox_value:\n",
    "        return gr.Dropdown(label=\"Выберите период\", choices=list(agregation_list.keys()), visible=True)\n",
    "    else:\n",
    "        # Скрыть выпадающий список, если чекбокс не отмечен\n",
    "        return gr.Dropdown(label=\"Выберите период\", choices=list(agregation_list.keys()), visible=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "lBVxK4wmkbL9"
   },
   "outputs": [],
   "source": [
    "def read_csv(file):\n",
    "    if file is None:\n",
    "        return gr.CheckboxGroup(choices=[], label=\"Выберите столбцы\")\n",
    "    try:\n",
    "        df = pd.read_csv(file)\n",
    "\n",
    "        # Определяем, есть ли столбец, который можно интерпретировать как дату\n",
    "        for column in df.columns:\n",
    "            # Попытка преобразования столбца в datetime, если не удается, игнорируем ошибки\n",
    "            try:\n",
    "                df[column] = pd.to_datetime(df[column])\n",
    "                df.rename(columns={column: 'date'}, inplace=True)\n",
    "                df.to_csv(file, index=False)\n",
    "                break  # Прекращаем цикл после первого успешного преобразования\n",
    "            except:\n",
    "                continue\n",
    "        # Исключаем столбец 'date' из списка столбцов\n",
    "        columns = [col for col in df.columns if col != 'date']\n",
    "\n",
    "\n",
    "        header_checkboxes.value = None\n",
    "        goal_radio_group.value = None\n",
    "\n",
    "        #clear_checkbox_group()\n",
    "        clear_goal_radio_group()\n",
    "\n",
    "        return gr.CheckboxGroup(choices=columns, label=\"Выберите столбцы\")\n",
    "    except Exception as e:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "S-TXk6cmI-Rl"
   },
   "outputs": [],
   "source": [
    "def update_fluger_checkboxes(add_fluger, selected_headers):\n",
    "    if add_fluger and selected_headers:\n",
    "        # Возвращаем список словарей, каждый из которых представляет чекбокс\n",
    "        return gr.CheckboxGroup(choices=selected_headers, label=\"Выберите столбцы для флюгера\", visible=True)\n",
    "    return gr.CheckboxGroup(choices=[], label=\"\", visible=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "tdXzcxZxxAeO"
   },
   "outputs": [],
   "source": [
    "def show_radio_buttons(is_checked):\n",
    "    # Если чекбокс активирован, показываем радиокнопки\n",
    "    if is_checked:\n",
    "        return gr.Radio(choices=list(transform_data_methods.keys()), label=\"Выберите тип\", visible=True)\n",
    "    # В противном случае скрываем радиокнопки\n",
    "    return gr.Radio(choices=[], visible=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "AbjHxeLHQiqS"
   },
   "outputs": [],
   "source": [
    "def update_radio_options(checkbox_selection):\n",
    "    # Возвращает список выборов, основанный на выборе в CheckboxGroup\n",
    "        return gr.Radio(choices=checkbox_selection, label=\"Выберите колонку\", visible=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "T48rhds514PN"
   },
   "outputs": [],
   "source": [
    "def clear_goal_radio_group():\n",
    "    return gr.Radio(choices=[],label=\"Выберите колонку\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "tt1KJHsn8Iqu"
   },
   "outputs": [],
   "source": [
    "def clear_checkbox_group():\n",
    "    return gr.CheckboxGroup(choices=[], label=\"Выберите столбцы\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "VwxvKjr6oslo"
   },
   "outputs": [],
   "source": [
    "def create_zip_archive(archive_path, dataset_processor, normalize_set):\n",
    "    with zipfile.ZipFile(archive_path, 'w') as zipf:\n",
    "        # Список файлов для архивации\n",
    "        files_to_zip = [\n",
    "            dataset_processor.file_name_train,\n",
    "            dataset_processor.file_name_test,\n",
    "            dataset_processor.file_name_json\n",
    "        ]\n",
    "\n",
    "        # Если используется нормализация, добавляем файл нормализатора\n",
    "        if normalize_set:\n",
    "            files_to_zip.append(dataset_processor.scaler_filename_y)\n",
    "            files_to_zip.append(dataset_processor.scaler_filename_x)\n",
    "\n",
    "        # Перебираем файлы и добавляем их в архив, если они существуют\n",
    "        for file_path in files_to_zip:\n",
    "            if os.path.exists(file_path):  # Проверка на существование файла\n",
    "                zipf.write(file_path, os.path.basename(file_path))\n",
    "            else:\n",
    "                print(f\"Файл {file_path} не найден и не будет добавлен в архив.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "ZBU1f5ZmW_cz"
   },
   "outputs": [],
   "source": [
    "def process_dataset(file,\n",
    "                    columns,\n",
    "                    fill_gaps,\n",
    "                    fill_method,\n",
    "                    remove_seasonality,\n",
    "                    remove_trend,\n",
    "                    fluger,\n",
    "                    fluger_columns,\n",
    "                    transform_date,\n",
    "                    date_transform_method,\n",
    "                    agregate_data,\n",
    "                    agregate_method,\n",
    "                    normalize_set,\n",
    "                    normalize_method,\n",
    "                    goal_radio_group,\n",
    "                    split_percentage):\n",
    "    # Создаем объект класса\n",
    "    dataset_processor = MakeMyDataSet()\n",
    "    # Обнуляем applied_methods\n",
    "    dataset_processor.applied_methods = {}\n",
    "\n",
    "    # Устанавливаем входной и выходной файлы\n",
    "    dataset_processor.setNameInputFile(file.name)\n",
    "    prefix = \"rez_\"\n",
    "    dataset_processor.setNameOutputFile(prefix)\n",
    "\n",
    "    # Выбор столбцов и создание нового датасета\n",
    "    dataset_processor.makeNewFileDataset(columns)\n",
    "\n",
    "    # Применяем выбранные методы\n",
    "\n",
    "    #заполняем пропуски\n",
    "    if fill_gaps:\n",
    "        # Преобразование метки метода в его ключ\n",
    "        fill_method_key = methods.get(fill_method, None)\n",
    "        dataset_processor.fillInterpol(dataset_processor._output_file, fill_method_key)\n",
    "\n",
    "    #добавить столбцы к флюгеру\n",
    "    if fluger:\n",
    "        dataset_processor.makeColumnsVane(dataset_processor._output_file, fluger_columns)\n",
    "\n",
    "    #добавить столбцы к дате\n",
    "    if transform_date:\n",
    "        date_transform_method_key = transform_data_methods.get(date_transform_method, None)\n",
    "        dataset_processor.transformColumnData(dataset_processor._output_file, date_transform_method_key)\n",
    "\n",
    "    #агрегация данных\n",
    "    if agregate_data:\n",
    "        agregate_key = agregation_list.get(agregate_method, None)\n",
    "        dataset_processor.agregateData(dataset_processor._output_file, agregate_key)\n",
    "\n",
    "    # Разделение датасета на тренировочную и тестовую выборки\n",
    "    dataset_processor.splitData(test_size=(100 - split_percentage) / 100)\n",
    "\n",
    "    #убрать сезонность\n",
    "    if remove_seasonality:\n",
    "        dataset_processor.deleteSeason(dataset_processor.file_name_train)\n",
    "        dataset_processor.deleteSeasonForTest(dataset_processor.file_name_test)\n",
    "\n",
    "    #убрать тренд\n",
    "    if remove_trend:\n",
    "        #dataset_processor.deleteTrend(dataset_processor.file_name_train)\n",
    "        #dataset_processor.deleteTrend(dataset_processor.file_name_test)\n",
    "        dataset_processor.applyDiffToData(dataset_processor.file_name_train, is_write = 0)\n",
    "        dataset_processor.applyDiffToData(dataset_processor.file_name_test, is_write =1)\n",
    "\n",
    "\n",
    "    if normalize_set:\n",
    "       # Преобразование метки метода в его ключ\n",
    "       normalize_method_key = normalize.get(normalize_method, None)\n",
    "       dataset_processor.normalizeData(dataset_processor.file_name_train,\n",
    "                                       goal_radio_group,\n",
    "                                       normalize_method_key)\n",
    "       dataset_processor.makeNormalizeForTest(dataset_processor.file_name_test,\n",
    "                                              goal_radio_group)\n",
    "\n",
    "    dataset_processor.add_goal_column_name_to_method(goal_radio_group)\n",
    "  \n",
    "    dataset_processor.save_methods_to_json(dataset_processor.file_name_json)\n",
    "    # Удаляем дату из данных\n",
    "    dataset_processor.deleteDateFromDataset(dataset_processor.file_name_train)\n",
    "    #dataset_processor.deleteDateFromDataset(dataset_processor.file_name_test)\n",
    "\n",
    "    # Создаем архив с файлами для скачивания\n",
    "    archive_path = \"files_to_download.zip\"\n",
    "    create_zip_archive(archive_path, dataset_processor, normalize_set)\n",
    "    return archive_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "rCcZuNIhqsdS",
    "outputId": "88c9cffa-8e22-48ff-8c7f-25579d7a0df9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTANT: You are using gradio version 4.21.0, however version 4.29.0 is available, please upgrade.\n",
      "--------\n",
      "Найден JSON файл: temp\\methods_info.json\n",
      "Loaded sampling_window_size: 10\n",
      "Loaded test_data_offset: 10\n",
      "Loaded column_name_goalt: Anem_40S_10_avg\n",
      "Применение normalizeData_y с параметрами {'method': 'minmax', 'scaler_file': 'scaler_y.save'}...\n",
      "Index(['Anem_40S_10_avg'], dtype='object')\n",
      "Данные успешно восстановлены из нормализованной формы.\n",
      "Найден файл модели: temp\\my_model_cnn.keras\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "2860 2860\n",
      "Средняя абсолютная ошибка (MAE): 0.024337350552486894\n",
      "Среднеквадратичная ошибка (MSE): 0.0011565265645214777\n",
      "Loaded sampling_window_size: 10\n",
      "Loaded test_data_offset: 10\n",
      "Loaded column_name_goalt: Anem_40S_10_avg\n",
      "Применение normalizeData_y с параметрами {'method': 'minmax', 'scaler_file': 'scaler_y.save'}...\n",
      "Index(['Anem_40S_10_avg'], dtype='object')\n",
      "Данные успешно восстановлены из нормализованной формы.\n",
      "Loaded sampling_window_size: 10\n",
      "Loaded test_data_offset: 10\n",
      "Loaded column_name_goalt: Anem_40S_10_avg\n",
      "Применение normalizeData_y с параметрами {'method': 'minmax', 'scaler_file': 'scaler_y.save'}...\n",
      "Index(['Anem_40S_10_avg'], dtype='object')\n",
      "Данные успешно восстановлены из нормализованной формы.\n"
     ]
    }
   ],
   "source": [
    "# Элементы интерфейса\n",
    "with gr.Blocks(css=css) as demo:\n",
    "  with gr.Tabs():\n",
    "################################################################################1-я вкладка\n",
    "    with gr.Tab(label = \"Прогноз\"):\n",
    "      with gr.Row():\n",
    "            with gr.Column():\n",
    "              with gr.Row():\n",
    "                  gr.Markdown(\"### Выбрать вышку\")\n",
    "              with gr.Row():\n",
    "                  dropdown_towers = gr.Dropdown(label=\"\", choices=list(models.keys()))\n",
    "\n",
    "            with gr.Column():\n",
    "              with gr.Row():\n",
    "                  gr.Markdown(\"### Выбрать модель\")\n",
    "              with gr.Row():\n",
    "                  dropdown_models = gr.Dropdown(label=\" \", choices=[])\n",
    "\n",
    "            with gr.Column():\n",
    "              with gr.Row():\n",
    "                gr.Markdown(\"### Выбрать файл данных\")\n",
    "              with gr.Row():\n",
    "                file_input = gr.File(label=\"Загрузить файл\", elem_classes=\"small-file-input\")\n",
    "              with gr.Row():\n",
    "                load_button = gr.Button(\"Загрузить данные в систему\", elem_classes=\"button_my\")\n",
    "\n",
    "      with gr.Row():\n",
    "              gr.Markdown(\"## Входные данные\")\n",
    "      with gr.Row():\n",
    "              load_button.click(fn=load_dataset,\n",
    "                                inputs=[dropdown_towers, dropdown_models, file_input],\n",
    "                                outputs=gr.Plot(label=\"Результат загрузки\"))\n",
    "      with gr.Row():\n",
    "              gr.Markdown(\"## Настройки прогноза\")\n",
    "      with gr.Row():\n",
    "          with gr.Column():\n",
    "              duration_slider = gr.Slider(minimum=1, maximum=365, label=\"Длительность прогноза, день\")\n",
    "          with gr.Column():\n",
    "              predict_button = gr.Button(\"Начать прогноз\", elem_classes=\"button_my\")\n",
    "      with gr.Row():\n",
    "              gr.Markdown(\"## Предсказание\")\n",
    "      with gr.Row():\n",
    "              output_plot = gr.Plot(label=\"График прогноза\")\n",
    "\n",
    "      # Размещение текстовых полей на отдельной строке\n",
    "      with gr.Row():\n",
    "        with gr.Column():\n",
    "            with gr.Row():\n",
    "                gr.Markdown(\"### MAE\")\n",
    "            with gr.Row():\n",
    "                output_mae = gr.Textbox(label=\"\")\n",
    "        with gr.Column():\n",
    "            with gr.Row():\n",
    "                gr.Markdown(\"### MSE\")\n",
    "            with gr.Row():\n",
    "                output_mse = gr.Textbox(label=\"\")\n",
    "        #with gr.Column():\n",
    "        #    with gr.Row():\n",
    "        #        gr.Markdown(\"### MAPE\")\n",
    "        #    with gr.Row():\n",
    "        #        output_mape = gr.Textbox(label=\"\")\n",
    "\n",
    "\n",
    "    # Настройка действия для кнопки\n",
    "    predict_button.click(fn=make_prediction,\n",
    "                         inputs=[duration_slider],\n",
    "                         outputs=[output_plot, output_mae, output_mse])\n",
    "\n",
    "     # Обработчик событий изменения выбора вышки\n",
    "    dropdown_towers.change(fn=update_models_dropdown, inputs=dropdown_towers, outputs=dropdown_models)\n",
    "################################################################################2-я вкладка\n",
    "    with gr.Tab(label = \"Подготовка датасета\", elem_classes=\"tab_my\"):\n",
    "      with gr.Row():\n",
    "          with gr.Column():\n",
    "              with gr.Row():\n",
    "                gr.Markdown(\"### Выбрать файл данных\")\n",
    "              with gr.Row():\n",
    "                file_input2 = gr.File(label=\"Загрузить файл\", elem_classes=\"small-file-input\")\n",
    "              with gr.Row():\n",
    "                load_button2 = gr.Button(\"Загрузить\", elem_classes=\"button_my\")\n",
    "              # Динамическое обновление для чекбоксов\n",
    "              with gr.Row():\n",
    "                  # Пустой контейнер для чекбоксов\n",
    "                  header_checkboxes = gr.CheckboxGroup(choices=[], label=\"Выберите столбцы\", interactive = True)  # Группа для динамически создаваемых чекбоксов\n",
    "\n",
    "          with gr.Column():\n",
    "                with gr.Row():\n",
    "                     gr.Markdown(\"### Выбрать методы подготовки датасета\")\n",
    "\n",
    "                with gr.Row():\n",
    "                  with gr.Column():\n",
    "                     fill_gaps_checkbox = gr.Checkbox(label=\"Заполнить пропуски\")\n",
    "                  with gr.Column():\n",
    "                     methods_dropdown = gr.Dropdown(label=\"Выберите метод\", choices=[], visible=False, interactive = True)\n",
    "\n",
    "                with gr.Row():\n",
    "                    remove_seasonality_checkbox = gr.Checkbox(label=\"Убрать сезонность\")\n",
    "\n",
    "                with gr.Row():\n",
    "                    remove_trend_checkbox = gr.Checkbox(label=\"Убрать тренд\")\n",
    "\n",
    "                with gr.Row():\n",
    "                  with gr.Column():\n",
    "                    fluger_checkbox = gr.Checkbox(label=\"Добавить столбцы для флюгера\")\n",
    "                  with gr.Column():\n",
    "                    fluger_checkboxes_group = gr.CheckboxGroup(choices=[], label=\"Выберите столбцы для флюгера\", visible=False, interactive=True)\n",
    "\n",
    "                with gr.Row():\n",
    "                    with gr.Column():\n",
    "                        transform_data_checkbox = gr.Checkbox(label=\"Трансформировать дату\")\n",
    "                    with gr.Column():\n",
    "                        # Контейнер для радиокнопок\n",
    "                        radio_buttons = gr.Radio(choices=[], visible=False, interactive = True)\n",
    "\n",
    "                with gr.Row():\n",
    "                    with gr.Column():\n",
    "                        agregation_checkbox = gr.Checkbox(label=\"Агрегировать данные\")\n",
    "                    with gr.Column():\n",
    "                        agregation_dropdown = gr.Dropdown(label=\"Выберите период\", choices=[], visible=False, interactive = True)\n",
    "\n",
    "                with gr.Row():\n",
    "                    with gr.Column():\n",
    "                        normalize_checkbox = gr.Checkbox(label=\"Нормализовать данные\")\n",
    "                    with gr.Column():\n",
    "                        normalize_dropdown = gr.Dropdown(label=\"Выберите scaler\", choices=[], visible=False, interactive = True)\n",
    "\n",
    "\n",
    "                with gr.Row():\n",
    "                        gr.Markdown(\"### Выбрать целевой ряд\")\n",
    "                        goal_radio_group = gr.Radio(label=\" \", visible=True, interactive = True)\n",
    "\n",
    "                with gr.Row():\n",
    "                    dataset_slider = gr.Slider(minimum=1, maximum=100, label=\"Разделение датасета на train/test, %\", interactive = True)\n",
    "                with gr.Row():\n",
    "                    make_dataset = gr.Button(\"Создать датасет\", elem_classes=\"button_my\")\n",
    "                with gr.Row():\n",
    "                    download_button = gr.File(label=\"Скачать датасет\", interactive=False, elem_classes=\"small-file-input\")\n",
    "\n",
    "\n",
    "     # При изменении состояния чекбокса вызывается функция update_dropdown_visibility\n",
    "    fill_gaps_checkbox.change(fn=show_methods, inputs=fill_gaps_checkbox, outputs=methods_dropdown)\n",
    "    normalize_checkbox.change(fn=show_normalize, inputs=normalize_checkbox, outputs=normalize_dropdown)\n",
    "\n",
    "    # Обработчик для кнопки \"Загрузить\"\n",
    "    load_button2.click(fn=clear_goal_radio_group, outputs=goal_radio_group)\n",
    "    load_button2.click(fn=clear_checkbox_group, outputs=header_checkboxes)\n",
    "\n",
    "    load_button2.click(fn=read_csv, inputs=file_input2, outputs=header_checkboxes)\n",
    "\n",
    "    agregation_checkbox.change(fn=show_agregation, inputs=agregation_checkbox, outputs=agregation_dropdown)\n",
    "    # Связываем чекбокс \"Добавить столбцы для флюгера\" с функцией update_fluger_checkboxes\n",
    "    fluger_checkbox.change(fn=update_fluger_checkboxes, inputs=[fluger_checkbox, header_checkboxes], outputs=fluger_checkboxes_group)\n",
    "    # Обновление радиокнопок на основе выбора в CheckboxGroup\n",
    "    header_checkboxes.change(fn=update_radio_options, inputs=header_checkboxes, outputs=goal_radio_group)\n",
    "\n",
    "\n",
    "    transform_data_checkbox.change(fn=show_radio_buttons, inputs=transform_data_checkbox, outputs=radio_buttons)\n",
    "    make_dataset.click(fn=process_dataset,\n",
    "                       inputs=[file_input2,\n",
    "                               header_checkboxes,\n",
    "                               fill_gaps_checkbox,\n",
    "                               methods_dropdown,\n",
    "                               remove_seasonality_checkbox,\n",
    "                               remove_trend_checkbox,\n",
    "                               fluger_checkbox,\n",
    "                               fluger_checkboxes_group,\n",
    "                               transform_data_checkbox,\n",
    "                               radio_buttons,\n",
    "                               agregation_checkbox,\n",
    "                               agregation_dropdown,\n",
    "                               normalize_checkbox,\n",
    "                               normalize_dropdown,\n",
    "                               goal_radio_group,\n",
    "                               dataset_slider],\n",
    "                       outputs=download_button)\n",
    "# Запуск интерфейса\n",
    "demo.launch(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zz6Q59fJrNST",
    "outputId": "6f799fc8-cfe6-4096-ae12-98b9d2df14d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON файл успешно обновлен.\n"
     ]
    }
   ],
   "source": [
    "json_file_path = 'd:/marina/TEST_WIND_ust_kam/methods_info.json'\n",
    "\n",
    "# Чтение JSON файла\n",
    "with open(json_file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Модификация данных JSON\n",
    "data['sampling_window_size'] = 10  # Пример размера окна выборки\n",
    "data['test_data_offset'] = 10       # Пример размера окна отступа в тестовых данных\n",
    "\n",
    "# Сохранение изменений в JSON файл\n",
    "with open(json_file_path, 'w') as file:\n",
    "    json.dump(data, file, indent=4)  # Использование indent для улучшения читаемости\n",
    "\n",
    "print(\"JSON файл успешно обновлен.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ekg68HkOwWjC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
